#!/usr/bin/env python3
"""
Performance benchmark script for measuring current system baseline.

This script provides comprehensive performance measurement and baseline
establishment for the Civitai downloader system.
"""

import argparse
import asyncio
import json
import sys
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from civitai_dl.config import DownloadConfig
from civitai_dl.services.download_service import DownloadService
from civitai_dl.monitoring.performance_monitor import PerformanceMonitor
from civitai_dl.monitoring.health_monitor import HealthMonitor
from civitai_dl.monitoring.metrics_collector import MetricsCollector


class BenchmarkRunner:
    """Performance benchmark execution and analysis."""
    
    def __init__(self, config: DownloadConfig):
        self.config = config
        self.output_dir = Path(config.output_dir) 
        self.benchmark_dir = self.output_dir / "benchmarks"
        self.benchmark_dir.mkdir(exist_ok=True)
        
        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.performance_monitor = PerformanceMonitor(self.output_dir)
        self.health_monitor = HealthMonitor(config, self.output_dir)
        self.metrics_collector = MetricsCollector(self.output_dir)
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹
        self.download_service = DownloadService(config)
        
    def run_baseline_benchmark(
        self, 
        test_users: List[str],
        duration_minutes: Optional[int] = None,
        max_models: Optional[int] = None
    ) -> Dict[str, Any]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½æ¸¬å®šå®Ÿè¡Œ."""
        print("ğŸš€ Starting baseline performance benchmark")
        print(f"   Test users: {len(test_users)}")
        if duration_minutes:
            print(f"   Duration limit: {duration_minutes} minutes")
        if max_models:
            print(f"   Model limit: {max_models} models")
            
        start_time = time.time()
        benchmark_id = str(uuid.uuid4())
        
        # å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯é–‹å§‹
        health_task = None
        if duration_minutes:
            health_task = asyncio.create_task(
                self.health_monitor.continuous_monitoring(duration_minutes / 60)
            )
        
        results = {
            "benchmark_id": benchmark_id,
            "start_time": datetime.utcnow().isoformat(),
            "configuration": {
                "test_users": test_users,
                "duration_minutes": duration_minutes,
                "max_models": max_models,
                "output_dir": str(self.output_dir),
                "production_mode": not self.config.is_test
            },
            "user_results": [],
            "overall_performance": {},
            "health_status": {},
            "recommendations": []
        }
        
        try:
            total_models_processed = 0
            
            for i, username in enumerate(test_users, 1):
                print(f"\nğŸ“¦ Processing user {i}/{len(test_users)}: {username}")
                
                # æ™‚é–“åˆ¶é™ãƒã‚§ãƒƒã‚¯
                if duration_minutes:
                    elapsed_minutes = (time.time() - start_time) / 60
                    if elapsed_minutes >= duration_minutes:
                        print(f"â° Time limit reached: {elapsed_minutes:.1f} minutes")
                        break
                
                # ãƒ¢ãƒ‡ãƒ«æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
                if max_models and total_models_processed >= max_models:
                    print(f"ğŸ“Š Model limit reached: {total_models_processed} models")
                    break
                    
                user_start_time = time.time()
                
                try:
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
                    user_result = self.download_service.download_user_models(username)
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²
                    user_duration = time.time() - user_start_time
                    models_processed = user_result.get("total_models", 0)
                    total_models_processed += models_processed
                    
                    user_benchmark = {
                        "username": username,
                        "success": user_result.get("success", False),
                        "duration_minutes": user_duration / 60,
                        "total_models": models_processed,
                        "successful_downloads": user_result.get("successful_downloads", 0),
                        "failed_downloads": user_result.get("failed_downloads", 0),
                        "models_per_minute": models_processed / (user_duration / 60) if user_duration > 0 else 0
                    }
                    
                    results["user_results"].append(user_benchmark)
                    
                    print(f"   âœ… Completed: {models_processed} models in {user_duration/60:.1f}min")
                    
                except Exception as e:
                    print(f"   âŒ Failed: {e}")
                    results["user_results"].append({
                        "username": username,
                        "success": False,
                        "error": str(e),
                        "duration_minutes": (time.time() - user_start_time) / 60
                    })
                    
                # ä¸­é–“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                self.metrics_collector.record_metrics_snapshot()
                
            # æœ€çµ‚æ€§èƒ½ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            end_time = time.time()
            total_duration = end_time - start_time
            
            results["end_time"] = datetime.utcnow().isoformat()
            results["total_duration_minutes"] = total_duration / 60
            results["total_models_processed"] = total_models_processed
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
            results["overall_performance"] = self._calculate_performance_stats(results["user_results"])
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦ç´„
            results["metrics_summary"] = self.metrics_collector.generate_performance_summary()
            
            # å¥å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆ
            results["health_status"] = self.health_monitor.generate_health_report()
            
            # æ¨å¥¨äº‹é …ç”Ÿæˆ
            results["recommendations"] = self._generate_benchmark_recommendations(results)
            
            print(f"\nğŸ‰ Benchmark completed!")
            print(f"   Duration: {total_duration/60:.1f} minutes")
            print(f"   Models processed: {total_models_processed}")
            print(f"   Average speed: {total_models_processed/(total_duration/60):.1f} models/min")
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Benchmark interrupted by user")
            results["interrupted"] = True
            results["end_time"] = datetime.utcnow().isoformat()
            
        except Exception as e:
            print(f"\nâŒ Benchmark failed: {e}")
            results["error"] = str(e)
            results["end_time"] = datetime.utcnow().isoformat()
            
        finally:
            # å¥å…¨æ€§ç›£è¦–åœæ­¢
            if health_task and not health_task.done():
                health_task.cancel()
                try:
                    await health_task
                except asyncio.CancelledError:
                    pass
                    
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜
        self._save_benchmark_results(results)
        
        return results
        
    def _calculate_performance_stats(self, user_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆè¨ˆç®—."""
        successful_users = [r for r in user_results if r.get("success", False)]
        
        if not successful_users:
            return {
                "total_users": len(user_results),
                "successful_users": 0,
                "success_rate": 0.0,
                "average_models_per_user": 0.0,
                "average_duration_per_user_minutes": 0.0,
                "total_throughput_models_per_minute": 0.0
            }
            
        total_models = sum(r.get("total_models", 0) for r in successful_users)
        total_duration_minutes = sum(r.get("duration_minutes", 0) for r in successful_users)
        
        return {
            "total_users": len(user_results),
            "successful_users": len(successful_users),
            "success_rate": len(successful_users) / len(user_results) * 100,
            "total_models": total_models,
            "average_models_per_user": total_models / len(successful_users),
            "average_duration_per_user_minutes": total_duration_minutes / len(successful_users),
            "total_throughput_models_per_minute": total_models / total_duration_minutes if total_duration_minutes > 0 else 0
        }
        
    def _generate_benchmark_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ç”Ÿæˆ."""
        recommendations = []
        
        overall_perf = results.get("overall_performance", {})
        metrics_summary = results.get("metrics_summary", {})
        
        # æˆåŠŸç‡ãƒã‚§ãƒƒã‚¯
        success_rate = overall_perf.get("success_rate", 0)
        if success_rate < 95:
            recommendations.append(
                f"Low success rate ({success_rate:.1f}%). "
                "Consider implementing additional error handling and retry logic."
            )
            
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒã‚§ãƒƒã‚¯
        throughput = overall_perf.get("total_throughput_models_per_minute", 0)
        if throughput < 5:  # 5ãƒ¢ãƒ‡ãƒ«/åˆ†æœªæº€
            recommendations.append(
                f"Low throughput ({throughput:.1f} models/min). "
                "Consider optimizing download speed or implementing limited concurrency."
            )
            
        # ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯
        reliability = metrics_summary.get("reliability", {})
        error_rate = reliability.get("error_rate_percent", 0)
        if error_rate > 5:
            recommendations.append(
                f"High error rate ({error_rate:.1f}%). "
                "Review error logs and implement more robust error handling."
            )
            
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç‡ãƒã‚§ãƒƒã‚¯
        timeout_rate = reliability.get("timeout_rate_percent", 0)
        if timeout_rate > 1:
            recommendations.append(
                f"High timeout rate ({timeout_rate:.1f}%). "
                "Consider implementing adaptive timeout handling."
            )
            
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        resource_usage = metrics_summary.get("resource_usage", {})
        memory_growth = resource_usage.get("memory_growth_rate_mb_per_hour", 0)
        if memory_growth > 50:  # 50MB/æ™‚é–“ä»¥ä¸Šã®å¢—åŠ 
            recommendations.append(
                f"Memory growth detected ({memory_growth:.1f} MB/hour). "
                "Investigate potential memory leaks."
            )
            
        if not recommendations:
            recommendations.append("System performance is within acceptable ranges.")
            
        return recommendations
        
    def _save_benchmark_results(self, results: Dict[str, Any]) -> Path:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜."""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        filename = f"baseline_benchmark_{timestamp}.json"
        filepath = self.benchmark_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ“Š Benchmark results saved: {filepath}")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚‚ã‚³ãƒ”ãƒ¼
        baseline_file = self.benchmark_dir / "baseline.json"
        with open(baseline_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        return filepath
        
    def compare_benchmarks(self, baseline_file: Path, current_results: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒåˆ†æ."""
        if not baseline_file.exists():
            return {"error": "Baseline file not found"}
            
        try:
            with open(baseline_file, 'r', encoding='utf-8') as f:
                baseline_results = json.load(f)
                
        except (json.JSONDecodeError, IOError) as e:
            return {"error": f"Failed to load baseline: {e}"}
            
        baseline_perf = baseline_results.get("overall_performance", {})
        current_perf = current_results.get("overall_performance", {})
        
        def calculate_change(current_val, baseline_val):
            if baseline_val == 0:
                return 0.0
            return ((current_val - baseline_val) / baseline_val) * 100
            
        comparison = {
            "baseline_date": baseline_results.get("start_time"),
            "current_date": current_results.get("start_time"),
            "performance_changes": {
                "success_rate_change": calculate_change(
                    current_perf.get("success_rate", 0),
                    baseline_perf.get("success_rate", 0)
                ),
                "throughput_change": calculate_change(
                    current_perf.get("total_throughput_models_per_minute", 0),
                    baseline_perf.get("total_throughput_models_per_minute", 0)
                ),
                "duration_change": calculate_change(
                    current_perf.get("average_duration_per_user_minutes", 0),
                    baseline_perf.get("average_duration_per_user_minutes", 0)
                )
            },
            "interpretation": self._interpret_comparison_results(
                current_perf, baseline_perf
            )
        }
        
        return comparison
        
    def _interpret_comparison_results(
        self, 
        current_perf: Dict[str, Any], 
        baseline_perf: Dict[str, Any]
    ) -> List[str]:
        """æ¯”è¼ƒçµæœã®è§£é‡ˆ."""
        interpretations = []
        
        current_throughput = current_perf.get("total_throughput_models_per_minute", 0)
        baseline_throughput = baseline_perf.get("total_throughput_models_per_minute", 0)
        
        if baseline_throughput > 0:
            throughput_change = (current_throughput - baseline_throughput) / baseline_throughput * 100
            
            if throughput_change > 10:
                interpretations.append(
                    f"âœ… Significant performance improvement: {throughput_change:.1f}% faster"
                )
            elif throughput_change > 5:
                interpretations.append(
                    f"âœ… Moderate performance improvement: {throughput_change:.1f}% faster"
                )
            elif throughput_change > -5:
                interpretations.append(
                    "â¡ï¸ Performance is stable (within 5% of baseline)"
                )
            elif throughput_change > -10:
                interpretations.append(
                    f"âš ï¸ Minor performance regression: {abs(throughput_change):.1f}% slower"
                )
            else:
                interpretations.append(
                    f"âŒ Significant performance regression: {abs(throughput_change):.1f}% slower"
                )
                
        return interpretations


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°."""
    parser = argparse.ArgumentParser(description="Civitai Downloader Performance Benchmark")
    parser.add_argument("--users", nargs="+", default=["alericiviai"], help="Test usernames")
    parser.add_argument("--duration", type=int, help="Duration limit in minutes")
    parser.add_argument("--max-models", type=int, help="Maximum models to process")
    parser.add_argument("--output-dir", type=str, default="./benchmarks", help="Output directory")
    parser.add_argument("--compare-with", type=str, help="Baseline file to compare with")
    parser.add_argument("--is-test", action="store_true", help="Run in test mode")
    
    args = parser.parse_args()
    
    # è¨­å®šä½œæˆ
    config = DownloadConfig(
        is_test=args.is_test,
        production_root=args.output_dir if not args.is_test else "./test_benchmarks"
    )
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    runner = BenchmarkRunner(config)
    
    print(f"ğŸ”§ Configuration:")
    print(f"   Test mode: {config.is_test}")
    print(f"   Output directory: {config.output_dir}")
    print(f"   Users: {args.users}")
    
    results = await runner.run_baseline_benchmark(
        test_users=args.users,
        duration_minutes=args.duration,
        max_models=args.max_models
    )
    
    # æ¯”è¼ƒåˆ†æï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if args.compare_with:
        baseline_file = Path(args.compare_with)
        comparison = runner.compare_benchmarks(baseline_file, results)
        
        print("\nğŸ“Š Comparison Results:")
        if "error" in comparison:
            print(f"   âŒ {comparison['error']}")
        else:
            changes = comparison["performance_changes"]
            print(f"   Success rate change: {changes['success_rate_change']:+.1f}%")
            print(f"   Throughput change: {changes['throughput_change']:+.1f}%")
            print(f"   Duration change: {changes['duration_change']:+.1f}%")
            
            for interpretation in comparison["interpretation"]:
                print(f"   {interpretation}")
    
    # æ¨å¥¨äº‹é …è¡¨ç¤º
    recommendations = results.get("recommendations", [])
    if recommendations:
        print("\nğŸ’¡ Recommendations:")
        for rec in recommendations:
            print(f"   â€¢ {rec}")
    
    print(f"\nğŸ“ Results saved to: {runner.benchmark_dir}")
    

if __name__ == "__main__":
    asyncio.run(main())